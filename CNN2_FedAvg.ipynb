{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshikapathak/Personalized-Federated-Learning/blob/main/Basic/FedAvg_and_FedSGD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d61vSHCXRgJr"
      },
      "source": [
        "## FedAvg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n"
      ],
      "metadata": {
        "id": "TxU2b6S56mcy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2OzI0JFzw9b4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset, TensorDataset\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "from torchsummary import summary"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data loaders"
      ],
      "metadata": {
        "id": "-HA1dOZQ6onj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "F3y6QJjtw9cB"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DrUyKL4Rw9cE"
      },
      "outputs": [],
      "source": [
        "def iid_loader(dataset, num_clients=100, batch_size=10):\n",
        "    total = len(dataset)\n",
        "    indices = torch.randperm(total).tolist()\n",
        "    data_per_client = total // num_clients\n",
        "    client_indices = [indices[i * data_per_client:(i + 1) * data_per_client] for i in range(num_clients)]\n",
        "\n",
        "    client_loaders = [\n",
        "        DataLoader(Subset(dataset, indices), batch_size=batch_size, shuffle=True)\n",
        "        for indices in client_indices\n",
        "    ]\n",
        "    return client_loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "tCBjKKWew9cH"
      },
      "outputs": [],
      "source": [
        "def non_iid_loader(dataset, num_clients=100, batch_size=10, classes_per_user=2, high_prob=0.6, low_prob=0.4):\n",
        "    # Helper function to get number of classes and labels\n",
        "    def get_num_classes_samples(data):\n",
        "        if isinstance(data.targets, list):\n",
        "            labels = np.array(data.targets)\n",
        "        else:\n",
        "            labels = data.targets.numpy()\n",
        "        classes, num_samples = np.unique(labels, return_counts=True)\n",
        "        return len(classes), num_samples, labels\n",
        "\n",
        "    # Helper function to generate data split based on class partitions\n",
        "    def gen_data_split(num_samples, labels, class_partitions):\n",
        "        data_class_idx = {i: np.where(labels == i)[0] for i in range(num_classes)}\n",
        "        for idx_list in data_class_idx.values():\n",
        "            random.shuffle(idx_list)\n",
        "\n",
        "        user_data_idx = [[] for _ in range(num_clients)]\n",
        "        for usr_i in range(num_clients):\n",
        "            for c, p in zip(class_partitions['class'][usr_i], class_partitions['prob'][usr_i]):\n",
        "                end_idx = int(p * num_samples[c])\n",
        "                user_data_idx[usr_i].extend(data_class_idx[c][:end_idx])\n",
        "                data_class_idx[c] = data_class_idx[c][end_idx:]\n",
        "        return user_data_idx\n",
        "\n",
        "    num_classes, num_samples, labels = get_num_classes_samples(dataset)\n",
        "    count_per_class = (classes_per_user * num_clients) // num_classes\n",
        "\n",
        "    # Generating class partitions\n",
        "    class_dict = {i: {'prob': np.random.uniform(low_prob, high_prob, size=count_per_class).tolist()} for i in range(num_classes)}\n",
        "    for probs in class_dict.values():\n",
        "        total = sum(probs['prob'])\n",
        "        probs['prob'] = [p / total for p in probs['prob']]\n",
        "\n",
        "    # Assign classes and probabilities to each client\n",
        "    class_partitions = {'class': [], 'prob': []}\n",
        "    available_classes = list(range(num_classes)) * count_per_class\n",
        "    random.shuffle(available_classes)\n",
        "    for _ in range(num_clients):\n",
        "        client_classes = random.sample(available_classes, classes_per_user)\n",
        "        for c in client_classes:\n",
        "            available_classes.remove(c)\n",
        "        client_probs = [class_dict[c]['prob'].pop() for c in client_classes]\n",
        "        class_partitions['class'].append(client_classes)\n",
        "        class_partitions['prob'].append(client_probs)\n",
        "\n",
        "    # Generating data splits\n",
        "    user_data_idx = gen_data_split(num_samples, labels, class_partitions)\n",
        "\n",
        "    # Creating data loaders\n",
        "    client_data_loaders = []\n",
        "    for indices in user_data_idx:\n",
        "        subset = Subset(dataset, indices)\n",
        "        loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
        "        client_data_loaders.append(loader)\n",
        "\n",
        "    return client_data_loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42iInNj8w9cO"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gY4LY7hfw9cQ"
      },
      "outputs": [],
      "source": [
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Linear(28*28, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.flatten(1)\n",
        "        x = self.nn(x)\n",
        "        return x\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels=3, n_kernels=16, out_dim=10):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, n_kernels, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(n_kernels, 2 * n_kernels, 5)\n",
        "        self.fc1 = nn.Linear(2 * n_kernels * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, out_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train fxns"
      ],
      "metadata": {
        "id": "p1BHgqud61r0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "MxlBbsYTw9cY"
      },
      "outputs": [],
      "source": [
        "def train(model, client_train_loader, epochs, optimizer, criterion):\n",
        "    model.train()\n",
        "    for _ in range(epochs):\n",
        "        for _, (data, target) in enumerate(client_train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    return model\n",
        "\n",
        "def evaluate(model, test_loader):\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            pred = output.argmax( dim=1)\n",
        "            correct += (pred == target).sum().item()\n",
        "            total += target.size(0)\n",
        "    acc = correct / total\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "o4UDZnovw9cZ"
      },
      "outputs": [],
      "source": [
        "def running_model_average(current, next, scale):\n",
        "    if current == None:\n",
        "        current = next\n",
        "        for key in current:\n",
        "            current[key] = current[key]*scale\n",
        "    else:\n",
        "        for key in current:\n",
        "            current[key] = current[key] + next[key]*scale\n",
        "    return current"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "KT4-dTk_w9ca"
      },
      "outputs": [],
      "source": [
        "def experiments(global_model, num_clients, client_frac, epochs, lr, train_loader, test_loader, max_rounds, criterion):\n",
        "    val_accuracy = []\n",
        "    train_accuracy = []\n",
        "    for t in range(max_rounds):\n",
        "        #print(\"starting round {}\".format(t))\n",
        "\n",
        "        clients = np.random.choice(np.arange(num_clients), int(num_clients*client_frac), replace = False)\n",
        "        #print(\"clients: \", clients)\n",
        "\n",
        "        global_model.eval()\n",
        "        global_model = global_model.to(device)\n",
        "        running_avg = None\n",
        "        for i, c in enumerate(clients):\n",
        "            #print(\"round {}, starting client {}/{}, id: {}\".format(t, i+1,num_clients*client_frac, c))\n",
        "            local_model = copy.deepcopy(global_model).to(device)\n",
        "            optimizer = torch.optim.SGD(local_model.parameters(), lr = lr)\n",
        "            local_model = train(local_model, train_loader[c], epochs=epochs, optimizer=optimizer, criterion = criterion)\n",
        "            running_avg = running_model_average(running_avg, local_model.state_dict(), 1/(num_clients*client_frac))\n",
        "\n",
        "        global_model.load_state_dict(running_avg)\n",
        "        val_acc = evaluate(global_model, test_loader)\n",
        "        print(\"round {}, validation acc: {}\".format(t, val_acc))\n",
        "        val_accuracy.append(val_acc)\n",
        "\n",
        "        # train_acc = evaluate(global_model, train_loader)\n",
        "        # train_accuracy.append(train_acc)\n",
        "\n",
        "    return np.array(val_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_final_loaders(train_data, num_clients, iid = 0, batch_size = 64, classes_per_user = 2):\n",
        "  if iid == 1: # want iid\n",
        "    train_loader = iid_loader(train_data, num_clients, batch_size)\n",
        "  else: # want non iid\n",
        "    train_loader = non_iid_loader(train_data, num_clients, batch_size, classes_per_user=classes_per_user)\n",
        "  test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "  return train_loader, test_loader"
      ],
      "metadata": {
        "id": "K6rUrTym9CE2"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "FiaaBKySw9cU"
      },
      "outputs": [],
      "source": [
        "# cnn = CNN(n_kernels = 11).to(device)\n",
        "# summary(cnn,(3,32,32))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Checking how many samples are allocated to each client\n",
        "\n",
        "# import numpy as np\n",
        "# import torch\n",
        "# from torch.utils.data import DataLoader, Subset\n",
        "# import random\n",
        "\n",
        "# # Call the function to get the list of DataLoader objects\n",
        "# client_data_loaders = non_iid_loader(train_data, num_clients=10, batch_size=64, classes_per_user=2)\n",
        "\n",
        "# # Iterate through each DataLoader in the list and print the number of samples it contains\n",
        "# for idx, loader in enumerate(client_data_loaders):\n",
        "#     # Each loader's dataset attribute is a Subset of the original dataset\n",
        "#     print(f\"Loader {idx+1}: {len(loader.dataset)} samples\")"
      ],
      "metadata": {
        "id": "5oU7q1Y25Ytx"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "GVWZPbD16IP2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgFp01L7w9cK",
        "outputId": "164be94f-97a9-4b70-a83b-f0cfe6ff5dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Data loading and transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_classes = 10\n",
        "client_frac = 1\n",
        "max_rounds = 1000\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "Sz_D5JaD9R60"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPT1: 10 clients, 5 internal epochs"
      ],
      "metadata": {
        "id": "X1Wo09RDAzHZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrhPgAOtw9cb",
        "outputId": "c89547db-3d28-401d-f1fc-d3284f888a27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "round 0, validation acc: 0.1001\n",
            "round 1, validation acc: 0.1262\n",
            "round 2, validation acc: 0.2696\n",
            "round 3, validation acc: 0.321\n"
          ]
        }
      ],
      "source": [
        "iid = 0 # non-iid\n",
        "num_clients = 10\n",
        "epochs = 5\n",
        "\n",
        "cnn = CNN(n_kernels=11) # cnn2\n",
        "cnn_niid_10_5 = copy.deepcopy(cnn)\n",
        "\n",
        "train_loader, test_loader = get_final_loaders(train_data, num_clients, iid = iid, batch_size = batch_size, classes_per_user = 2)\n",
        "acc_cnn_niid_10_5_val = experiments(cnn_niid_10_5, num_clients = num_clients, client_frac = client_frac, epochs= epochs, lr = 0.05, train_loader= train_loader, test_loader = test_loader, max_rounds=max_rounds, criterion=criterion)\n",
        "# Save the accuracy in an NPZ file\n",
        "np.savez(\"acc_cnn_niid_10_5.npz\", val_accuracy=acc_cnn_niid_10_5_val) # train_accuracy=acc_cnn_niid_10_5_train,\n",
        "print(\"Done\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPT2: 10 clients, 20 internal epochs"
      ],
      "metadata": {
        "id": "DypL15pTB4Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iid = 0 # non-iid\n",
        "num_clients = 10\n",
        "epochs = 20\n",
        "\n",
        "cnn = CNN(n_kernels=11) # cnn2\n",
        "cnn_niid_10_20 = copy.deepcopy(cnn)\n",
        "\n",
        "train_loader, test_loader = get_final_loaders(train_data, num_clients, iid = iid, batch_size = batch_size, classes_per_user = 2)\n",
        "acc_cnn_niid_10_20_val = experiments(cnn_niid_10_20, num_clients = num_clients, client_frac = client_frac, epochs= epochs, lr = 0.05, train_loader= train_loader, test_loader = test_loader, max_rounds=max_rounds, criterion=criterion)\n",
        "# Save the accuracy in an NPZ file\n",
        "np.savez(\"acc_cnn_niid_10_20.npz\", val_accuracy=acc_cnn_niid_10_20_val) # train_accuracy=acc_cnn_niid_10_5_train,\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "zy1R-59tB23e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPT3: 20 clients, 5 internal epochs"
      ],
      "metadata": {
        "id": "qUh293qrCIQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iid = 0 # non-iid\n",
        "num_clients = 20\n",
        "epochs = 5\n",
        "\n",
        "cnn = CNN(n_kernels=11) # cnn2\n",
        "cnn_niid_20_5 = copy.deepcopy(cnn)\n",
        "\n",
        "train_loader, test_loader = get_final_loaders(train_data, num_clients, iid = iid, batch_size = batch_size, classes_per_user = 2)\n",
        "acc_cnn_niid_20_5_val = experiments(cnn_niid_20_5, num_clients = num_clients, client_frac = client_frac, epochs= epochs, lr = 0.05, train_loader= train_loader, test_loader = test_loader, max_rounds=max_rounds, criterion=criterion)\n",
        "# Save the accuracy in an NPZ file\n",
        "np.savez(\"acc_cnn_niid_20_5.npz\", val_accuracy=acc_cnn_niid_20_5_val) # train_accuracy=acc_cnn_niid_10_5_train,\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "NxuC0LnvCHRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EXPT4: 20 clients, 20 internal epochs"
      ],
      "metadata": {
        "id": "QBIxE-ZACXbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iid = 0 # non-iid\n",
        "num_clients = 20\n",
        "epochs = 20\n",
        "\n",
        "cnn = CNN(n_kernels=11) # cnn2\n",
        "cnn_niid_20_20 = copy.deepcopy(cnn)\n",
        "\n",
        "train_loader, test_loader = get_final_loaders(train_data, num_clients, iid = iid, batch_size = batch_size, classes_per_user = 2)\n",
        "acc_cnn_niid_20_20_val = experiments(cnn_niid_20_20, num_clients = num_clients, client_frac = client_frac, epochs= epochs, lr = 0.05, train_loader= train_loader, test_loader = test_loader, max_rounds=max_rounds, criterion=criterion)\n",
        "# Save the accuracy in an NPZ file\n",
        "np.savez(\"acc_cnn_niid_20_20.npz\", val_accuracy=acc_cnn_niid_20_20_val) # train_accuracy=acc_cnn_niid_10_5_train,\n",
        "print(\"Done\")"
      ],
      "metadata": {
        "id": "RPquZCehCbTX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}